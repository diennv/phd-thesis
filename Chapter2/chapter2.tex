\chapter{Background}
%Before explaining the proposed VA design, the background information necessary to understanding VA architecture is described %briefly, followed by a short summary of state-of-art algorithms for object detection used in our proposed VA.
Moving object detection in videos has been received a lot of attention of video analytics researchers in recent years.
On both utilizing compressed and uncompressed video domain,  a lot of approaches already published to handle video moving object detection. In this chapter, we will introduce the current state-of-the-art solutions for moving object detection. In addition, the background information necessary to understanding VA architecture is described briefly.
\section{Surveillance Video Analytics Architectures and Challenges}
\begin{figure*}
\centering
\subfloat[]
{
    \includegraphics[scale=0.6]{Figures/Va-neta.png}
    \label{fig:va_a}
}
\subfloat[]
{
    \includegraphics[scale=0.6]{Figures/Va-netb.png}
    \label{fig:va_b}
}
\caption{Video Analytics Implementation, (a) Video management server based implementation, (b) Edge camera based implementation}
\label{fig:va_arch}
\end{figure*}
Video surveillance systems are normally built using the following main components: surveillance IP cameras, VMS, device storage and VA modules (optional). VA can be implemented in two main configurations, as discussed below.
\subsection{Edge Camera Based Deployment}
In this approach, the VA is deployed through a  camera device or video encoder \cite{chen2017smart} such as that shown in Figure \ref{fig:va_b}, which must have enough the processing power to run the VA functionality. Hence it is an expensive and challenging approach to wide-area video processing. This approach sound very ideally, however it does not handle satisfactorily in many cases because of lack sufficient processing power for highend VA requirements, and therefore, this approach has many drawbacks in real-world deployment. 
\subsection{Video Management Server Based Implementation}
In this approach, as shown in Figure \ref{fig:va_a}, VA is deployed at a powerful server which receives the video data from the surveillance cameras or from VMS, analyzes it, and issues analysis results. However, it has some challenges, which are listed below:
\begin{itemize}
\item “The VA server requires the video to be transmitted and therefore causes an increase in network traffic congestion. In detail, running video analytics by forwarding all video to the cloud overloads with the bandwidth constraints of some deployments, which do uploading all camera data. Each camera’s uplink bandwidth is limited, both by the physical constraints of network components such as: cable, switch, router.. Specifically, we consider large-scale deployments where each camera receives a bandwidth allocation of a few hundred kilobits per second, or less. For comparison, a low-quality H.264-encoded 1080p (1920×1080 pixels) stream is approximately 2 Mbits/s, an order of magnitude greater than our available uplink bandwidth. Yet, such low-quality data is often insufficient to perform accurate analysis: Modern 4K (3840×2160 pixels) cameras produce up to 30 to 40 Mbits/s. An edge-based filter answers this challenge with semantic filtering that uploads only frames that are relevant to applications”.
\item “The video data quality analyzed by the VA server is usually degraded because of lossly compression and transmission effects, and therefore”.
\item “The VA server is limited by its processing power, which makes it infeasible for large scale surveillance installations which deploy hundreds (and increasingly thousands) of cameras requiring a variety of VA functionalities. For example, a Full HD (1080P) stream at 30 FPS is $\approx$ 1.4 Gb/s when decompressed. Accomplishing VA at scale requires abundant compute, memory resources, so existing systems often perform this processing in the cloud, using GPUs”.
\end{itemize}
However, this approach is independent with surveillance camera sources, and is therefore applicable to most types of surveillance systems, and recent technological developments can reduce the effect of the above drawbacks. For example, the release of high definition video surveillance footage with resolutions up to 1080p will decrease the impact of the image quality degradation during codec processing and by releasing the new video coding standard as well as the transcoder \cite{thanh2019efficient}, high efficiency video coding (HEVC)\cite{sullivan2012overview} which has achieved approximately twice the standard compression\cite{ohm2012comparison} will decrease traffic in the network and at the central server, where powerful devices will have sufficient processing power to handle hundreds of camera. Moreover, some edge-based filtering approaches that
is designed to overcome these above issues \cite{canel2019scaling}\cite{li2020reducto}\cite{chen2015glimpse}. These approaches use edge-compute resources allocate with the cameras to identify the video sequences that are necessary to cloud applications and forward only that data for analyzing later. By this strategy, it helps to reduce the wide-area network links bandwidth. In this study, we present an edge-based filter which offers the benefits of ultilizing the both edge  and clound computing to video analytics processing.
\subsection{Video Codec and Video Coding Motion Vectors}
\subsubsection{Video Codec}
\begin{figure*}
\centering
 \includegraphics[width=0.8\linewidth]{Figures/encoder.png}
 \caption{Video encoder block diagram.}
 \label{fig:encoder}
\end{figure*}
The previous video coding standards, such as MPEG-1, MPEG-2, H.264 and H.265, are based on block-based motion compensation, transform, quantisation and entropy coding. “A video encoder as shown in Figure \ref{fig:encoder} consists of three main functional units: a temporal, a spatial and an entropy encoder. The input to the temporal model is an uncompressed video sequence. The temporal model tries to reduce temporal redundancy by finding the similarities between neighbouring video frames, normally by constructing a prediction of the current video frame. In MPEG-4 and H.264 codec, the prediction is formed from one or more previous and future frames and is improved by compensating for differences between the frames (motion compressed prediction). The output of the temporal model is a residual frames (created by subtracting operation the prediction from the actual current frame) and a set of model parameters, typicially a set of MVs describing how the motion was compressed. \\
The residual frame formes the input to the spatial model which makes use of similarities between neighbouring samples in the residual frame to reduce spatial redundancy. In MPEG-4 Visual and H.264 this is archieved by applying a transform to the residual samples and quantizing the results. The transform converts the samples into another domain in which they are represented by transform coefficients. The coefficiencents are quantised to remove insignificant values, leaving a small number of significant coefficients that provide a more compact representation of the residual frame. The output of the spatial model is a set of quantised transform coefficients.\\
The entropy encoder compressed the parameters of the temporal model (typically MVs) and the spatial model (coefficients). A compressed sequency consists of coded motion vector parameters, coded residual coefficients and header information.\\
At the decoder, a video frame is reconstructed from the compressed bitstream. The entropy decoder decoded the coefficients and motion vectors are decoded by an entropy decoder to reconstruct a version of the residual frame. The decoder uses the motion vector parameters, together with one or more previously decoded frames, to create a prediction of the current frame and the frame itself is reconstructed by adding the residual frame to this prediction”.
\subsubsection{Video Coding Motion Vectors and Blocked-based Motion Estimation}
	Changes through video frames may come from object motion (i.e, a human running), camera motion (zoom, rotation) and lighting condition changes.  These changes will make the pixel movements between frames. The trajectory of each pixel through video frames is able to estimate and generate a field of pixel trajectories known as optical flows. Optical flow gives the movement of a pixel in two consecutive frames. This measure of movement is presented by a vector where the magnitude of the vector represents the amount of motion and the angle of the vector specifies the direction of the motion. This vector is called motion vector. 
\begin{figure*}
\centering
 \includegraphics[width=0.8\linewidth]{Figures/opticalflow.jpeg}
 \caption{Motion Vector in Video Codec.}
 \label{fig:opticalflow}
\end{figure*}
\begin{figure*}
\centering
 \includegraphics[width=0.8\linewidth]{Figures/macroblock.png}
 \caption{Macroblock in Video Encoder.}
 \label{fig:macroblock}
\end{figure*}
Figure \ref{fig:opticalflow} shows the optical flow (or motion vector) field of an image.\\
 In video coding standard, the current frame is sub-devided in MxN blocks called macroblocks as shown in Figure \ref{fig:macroblock}. The following steps are processed for each block MxN samples in the current frame:
\begin{itemize}
\item “Searching an area in the reference frame (past or future frame) to look after a 'matching' MxN-sample region. This process is to find the best match and is known as motion estimation”.
\item “The selected candidatge region becomes the predictor for the current MxN block and is removed from the current block to form a residual MxN block (known as motion compensation)”.
\item “The residual block and the offset between the current block and the position of the candidate region (motion vector) is encoded and transmited”. 
\end{itemize}
\begin{figure*}
\centering
 \includegraphics[width=0.8\linewidth]{Figures/yuv420.png}
 \caption{Macroblock (4:2:0).}
 \label{fig:yuv420}
\end{figure*}
	In H264/AVC standard, the macroblock, coressponding to a block size of 16x16 pixel is the basic unit for motion compensated prediction. In the RGB colour space, each pixel is represented by three numbers indicating 3 basic colors of Red, Green and Blue. “However, human eyes are more sensitive to luminance ( brightness ) than to colour. Thus, data can be down sample by transforming the RGB to YCbCr which throw some color components without causing much visual distortion. In example, in the 4:2:0 sampling format, Y has double Cb and Cr each has half the horizontal and vertical resolution of Cb and cr. Figure \ref{fig:yuv420} shows YCbCr 4:2:0 format with a 16x16 pixel macroblock. The macroblock is divided into 8x8 sample blocks. In the RGB space, a macroblock has a total of 3x4 = 12 (8x8 sample blocks) ( four for each of the colors Red, Green and Blue ). However, there are only six sample blocks in YCbCr 4:2:0 format, four for the Y component and 1 for each of Cb and Cr component.”\\
  “In  motion compensation, the MVs of other neighboring blocks in the current frame or in the earlier coded frames \cite{laroche2008rd}, \cite{jiang2019spatial} are correlated with MV of a current block. While intra-picture prediction uses correlations between spatially neighboring samples, then inter-picture prediction makes use of temporal correlation between frames to derive a motion-compensated prediction (MCP) for a block of frame samples \cite{bross2014inter}.
\begin{figure*}
\centering
 \includegraphics[width=1.0\linewidth]{Figures/mv.png}
 \caption{ The eeference MV model in video coding.}
 \label{fig:mv}
\end{figure*}
For intra-prediction, we assume that neighboring blocks possibly correspond to the same moving object with similar motion and that motion  is continuous to change over time. As the result, using MV in neighboring blocks as predictor can reduce the size of the signaled motion vector difference. For this block-based MCP, a video frame is divided into rectangular blocks. Figure \ref{fig:mv} presents the novel concept of MCP with a translational motion model. Using this model, the position of the block in a previously decoded picture is represented by a motion vector $(\Delta x, \Delta y)$, where $\Delta x$ is the horizontal and $\Delta y$ the vertical displacement relative to the position of the current block. These motion vectors are coded by entropy coding and placed into a compressed bitstream to send to the video decoder side,  and the decoder will use these MVs, along with one or more previously decoded pictures, to reconstruct the current image. On the other hand, in H.264/AVC, each video sequence is devided into groups of pictures (GOP), comprising at least one intra code I frame, uni-directionally predicted P frames and bi-directionally predicted B frames. Normally, the first frame in a GOP is intra coded I frame and follows by P, B frames. I frame utilize raw data from camera, while other frames in a GOP use the predictive coding involved motion vectors displacement” (Nguyen Van Dien and Jaehyuk Choi, 2020, p.3). \\
%This is done by finding a displacement vector, i.e motion vector, for each block so that it optimises the rate-distortion requirements. As a result of the search criteria for 
\section{Compressed-Domain Based Moving Object Detection Using Motion Vector}
% Uncomment this line, when you have siunitx package loaded.
%The SI Units for dynamic viscosity is \si{\newton\second\per\metre\squared}.
%I'm going to randomly include a picture Figure~\ref{fig:minion}.


%If you have trouble viewing this document contact Krishna at: \href{mailto:kks32@cam.ac.uk}{kks32@cam.ac.uk} or raise an issue at \url{https://github.com/kks32/phd-thesis-template/}


%\begin{figure}[htbp!] 
%\centering    
%\includegraphics[width=1.0\textwidth]{minion}
%\caption[Minion]{This is just a long figure caption for the minion in Despicable Me from Pixar}
%\label{fig:minion}
%\end{figure}
 As mentioned in last section, MVs are extracted for each motion block between the current and referenece frames. By minimising the prediction residual, the MVs present the temporal displacement between the two block in the process of motion compression. As the result, MV information will follow the real motion of objects and can be used for tracking purpose.\\
“In recent researches, the compressed-domain based video analytics methods such as: \cite{bombardelli2018efficient},\cite{khatoonabadi2012video}. they are based on video coding features such as MVs, macroblock partition, and quantization coefficients, have been proposed. In \cite{bombardelli2018efficient}, the authors applied a probabilistic technique of computer vision for image separation, known as Graph Cut \cite{boykov2001fast}, modeling with MVs rather than pixels and adapted to the additional temporal dimension of video signals. Using MVs and a spatio-temporal Markov random field (ST-MRF) model that naturally integrates the spatial and temporal aspects of the object’s motion for tracking. In general, these approaches do not rely on pixels and studies by only using the codec’s MVs and block coding modes extracted bitstream through inexpensive partial decoding. In this manner, computing and storage requirements have been significantly reduced compared to pixel-domain” (Nguyen Van Dien and Jaehyuk Choi, 2020, p.4).\\

\begin{figure*}
\centering
\subfloat[]
{
    \includegraphics[width=0.5\linewidth]{Figures/noise.png}
}
\subfloat[]
{
    \includegraphics[width=0.5\linewidth]{Figures/156_mv_0.jpg}
}\\
\subfloat[]
{
    \includegraphics[width=0.5\linewidth]{Figures/3.jpg}
}
\subfloat[]
{
    \includegraphics[width=0.5\linewidth]{Figures/34.jpg}
}
 \caption{ The example of MV extraction.(a) Test video sequence from recorded camera, (b, c, d) Test video sequence from VIRAT dataset.}
 \label{fig:noise}
\end{figure*}

“The primary limitation of this approach is that it may lead to a noisy MV field that does not necessarily correspond with actual object movement of the object in the scene, as shown in Figure \ref{fig:noise}. The noisy MVs fail to provide useful information such as those attributed to illumination changes and background movement. The amount of noise MVs is relatively reduced compared to the correctly estimated MVs because noisy vectors are continuous and similar MVs from real moving objects. Another challenge of this approach is the lack of information about the object’s appearance such as color, edges and texture, because these features would require complete decoding of the compressed bitstream. In this study, our aim is to work in the compressed domain and uses only the MVs from the compressed bitstream to detect and track moving objects in video frames” (Nguyen Van Dien and Jaehyuk Choi, 2020, p.4).
%To apply the IoU-based object tracking algorithm \cite{rezatofighi2019generalized}, MVs are clustered into blobs and simply represented with object bounding boxes.

\section{Pixels-Domain Based Moving Object Detection using Deep Learning}
Because of watching the features that is extracted from the pixel data will be more reliable, the majority of moving object detections have been done in pixel domain, even it requires full decoding the video stream. In pixels domain, a VA server analyzes the R-G-B image to find the appearance objects and generating alerts. There are lot of drawback and exciting research problem. Howver, they are overcome by the recent advantages of hardware and deep learning \cite{zeng2018background},\cite{chen2017pixel},\cite{babaee2018deep},\cite{wang2017interactive},\cite{patil2018msfgnet},\cite{ou2019moving}. There are two main methods that are considered for moving object detection in pixels domain.
\subsection{The combination of background modeling and CNN based object classification}
\begin{figure*}
\centering
\subfloat[]
{
    \includegraphics[scale=0.5]{Figures/input_img.png}
}
\subfloat[]
{
    \includegraphics[scale=0.5]{Figures/fg_subtration.png}
}\\
\subfloat[]
{
    \includegraphics[scale=0.5]{Figures/blob_detection.png}
}
\subfloat[]
{
    \includegraphics[scale=0.5]{Figures/smoke_region.png}
}
\caption{Smoke detection pipeline: (a) input frame, (b) the foreground subtraction, (c) the blob detection, (d) the smoke candidates classification }
\label{fig:bgmethod}
\end{figure*}
\begin{figure*}
\centering
 \includegraphics[width=0.3\linewidth]{Figures/smoke.jpg}
 \caption{The pipeline of video- based smoke detection algorithm.}
 \label{fig:smoke}
\end{figure*}
The background subtraction approach \cite{lee2012adaptive}\cite{stauffer1999adaptive} “subtracts the current image and background image to eliminate the background, and then detects moving targets based on pixel clustering. This method is widely used for object detection. There are many methods that have been purposed for building the background model, e.g.”  \cite{lu2008improved} presented a novel real-time motion detection method that integrates the temporal differencing, double background filtering, optical flow, and morphological processing methods to obtain excellent performance. Moreover, the authors of \cite{stauffer1999adaptive} discussed modeling each pixel as a grouping of Gaussians using an on-line approximation to renew the model. “The Gaussian distributions of the adaptive mixture model were then assessed to determine which model can be presumed to be obtained from a background process. This method’s advantage is its simple implementation, low computational resource requirements, robustness in the presence of environmental noise, and dynamic background. However, it has limitations with shadows, background changes, as well as object localization and classification. Furthermore, object detection based on the background model cannot detect the individual objects in a group or a block object. In general, the aim of background subtraction is to separate foreground images from background ones in the form of blobs, followed by an object classification process” (Nguyen Van Dien and Jaehyuk Choi, 2020, p.5). The detected blobs then help classify each blob into subclasses, as shown in Figure \ref{fig:bgmethod}, which shows the complete process of this approach is to perform smoke detection with the detail of flow-char is drawn in Figure \ref{fig:smoke}. In this example, smoke classification was done by trained convolutional neural networks (CNN) model which was described in \cite{krizhevsky2017imagenet} for image classification. This a relatively new approach in machine learning, which ultilize CNNs to achive the very accurate classification of images. \cite{lecun2010convolutional}\cite{jarrett2009best}\cite{lee2009convolutional}. 
\begin{figure*}
\centering
 \includegraphics[width=0.8\linewidth]{Figures/AlexNet.png}
 \caption{Architecture of the Alexnet CNN model.}
 \label{fig:alex}
\end{figure*}
Figure \ref{fig:alex} shows the original architecture of the trained model, first five layer are convolutional and some of them are followed by max pooling layer. The next are three fully-connected layer, the last fully-connected layer computes class score of trained class labels.

The accuracy of CNN based model can be changed by varying their depth and breadth. For example, by combining CNN and transfer learning, the authors of \cite{hussain2018study} achieved an average accuracy of 70.1\% using the CIFAR-10 \cite{krizhevsky2009learning} dataset.

\subsection{Deep Learning based Moving Object Detection}
\label{subsec:frameworks}
\begin{figure*}
\centering
\subfloat[]
{
    \includegraphics[scale=0.3]{Figures/rcnn.png}
}
\subfloat[]
{
    \includegraphics[scale=0.45]{Figures/faster_rcnn.png}
}\\
\subfloat[]
{
    \includegraphics[scale=0.5]{Figures/fast_rcnn.png}
}
\caption{CNN based Object Detection Framework Structure.(a) RCNN, (b) Faster-RCNN, (c)Fast-RCNN.}
 \label{fig:cnn}
\end{figure*}

\begin{figure*}
\centering
 \includegraphics[width=0.8\linewidth]{Figures/yolo.png}
 \caption{YOLO Structer.}
 \label{fig:yolo}
\end{figure*}

Hybrid background subtraction and deep learning classification enhance the system’s accuracy; “however, there are issues with detection of individual objects in a group or blocked objects and the background changes by the lighting condition and the environmental noise” (Nguyen Van Dien and Jaehyuk Choi, 2020, p.5). Hence, the usage of deep learning is considered for robust object detection tasks. We have identified three primary object detection methods using deep learning:
\begin{itemize}
\item R-CNN model, Fast R-CNN model, Faster R-CNN model.
\item You Only Look Once (YOLO)
\item Single Shot Detectors (SSDs) 
\end{itemize}
“R-CNN \cite{girshick2014rich} uses selective search \cite{uijlings2013selective} to extract several regions, called region proposals, from the image; it then attempts to classify a large number of regions. Each region proposal is placed into CNNs to extract features, which are fed to a support vector machine to classify the presence of object within the candidate region proposal as shown in Figure \ref{fig:cnn}(a). The limitation of this method is that it requires a large amount of time to train and deploy networks because it has to classify multiple region proposals per image. Fast R-CNN \cite{girshick2015fast} solves the limitations of R-CNN by putting the entire image into the CNNs to generate a convolutional feature map and identify the region of proposals based on the map, thus reducing the number of classified region of proposals as drawn in Figure \ref{fig:cnn}(c). Both R-CNN and Fast R-CNN use selective search to identify the region proposals. Selective search is a slow and tedious process that affects the network’s performance. Faster R-CNN \cite{ren2015faster} was proposed to allow the network to learn the region proposals as shown in Figure \ref{fig:cnn}(b); it uses a separate network to predict the region proposals rather than use a selection search algorithm on the feature maps output using the CNN layer. YOLO \cite{redmon2016you} is an object detection system to detect object at real-time speed. Unlike R-CNN, Fast R-CNN and Faster R-CNN, which use regions to localize the object within the image, YOLO ultilize a single neural network to predict the bounding boxes and the class probabilities for these boxes during training and test periods. Hence, YOLO only examines the input picture once to predict the presence and location of objects. YOLO divides the input image into an SxS grid, and multiple bounding boxes can exist within each grid. For each bounding box, the network outputs a class probability and offset value for the bounding box” (Nguyen Van Dien and Jaehyuk Choi, 2020, p.7). The bounding boxes with class probabilities above a certain threshold value are then selected and used to locate the object within the image as shown in Figure \ref{fig:yolo}. “Specially, YOLOv3 network architecture has 24 convolutional layers, followed by two fully connected layers as shown in Figure \ref{fig:yolov3}. The initial convolutional layers of the network extract feature from the image, while the fully connected layers predict the output probabilities and coordinates. According to performance comparison in \cite{redmon2018yolov3}, YOLOv3 is faster (45 frames per second (FPS)) than the other object detection algorithms. Faster R-CNN is more accurate than YOLOv3 (a mean average precision (mAP) of 73.2, as compared to 63.4); however, YOLOv3 is considerably faster than Faster R-CNN (FPS of 45, as compared to 7). Therefore, SSDs  \cite{liu2016ssd} were released as a balance between these two methods. Compared to YOLO, an SSD runs an input image through a convolutional network only once and computes a feature map. Then, a small 3 × 3 sized convolutional kernels are run on this feature map to predict the bounding boxes and categorization probability. Moreover, SSD uses anchor boxes at various aspect ratios comparable to Faster-RCNN and learns the off-set to a certain extent compared to learning the box. The SSD is able to detect objects of multiple scales because every convolutional layer function at a diverse scale. Compared to the YOLOv3 method \cite{redmon2018yolov3}, the SSDs attain similar accuracy but run slower” (Nguyen Van Dien and Jaehyuk Choi, 2020, p.7). In this study, YOLOv3 was applied to robust human detection at cloud servers for our implementation. 

\begin{figure*}
\centering
 \includegraphics[width=1.0\linewidth]{Figures/yolov3.png}
 \caption{YOLOv3 Network Model.}
 \label{fig:yolov3}
\end{figure*}
