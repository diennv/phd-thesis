%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
%Here you make the strongest statement concerning observations. Highlight the information you want readers to remember. Explain how the results correlate with the problems you have indicated in the introduction. Describe all new things that are significant in finding a solution.

%The recommendations part is for giving advice and indicating other actions that will help to solve particular problems. Sound your own opinion about the direction of future research. Most of the time you have to write it. Just like a research proposal.

%Acknowledgments part is a paragraph where you mention everyone who helped you with composition. Place all cited and used information resources into one list or form an annotated bibliography if needed.
\chapter{Methodology}

% **************************** Define Graphics Path **************************
%\ifpdf
%    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
%\else
%    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
%\fi
In this section, the proposed edge-to-cloud system for surveillance video analytics applications is presented detaily. And then, the method for moving objects detection in compressed-domain is analyzed. In addtion, the performance evaluated model for video analytics platform is introduced. 

\section{The Edge-to-clound System Model for Surveillance Camera based Applications}
\begin{figure*}
\centering
 \includegraphics[width=1.0\linewidth]{Figures/arch.png}
 \caption{ Overview of the proposed edge-to-cloud system model.}
 \label{fig:arch}
\end{figure*}
There is a trend to forward computation from the network core to the edge where most of the data are generated. Edge computing has exhibited its potential in reducing the reaction time, minimizing bandwidth usage, and improving energy efficiency. Edge computing performs data processing at the “edge” of the network, close to the data source. For network cameras, audio and other sensors, there is a need to balance both cloud computing and edge computing domains to deliver refined, reliable, and usable data. For edge computing of delay-sensitive video tasks, a camera source node can offload its video task to nearby edge nodes via local wireless/optical networks and edge nodes are within the local communication range of the camera. The camera captures the video sequences and divides each of them into multiple video chunks, compresses these video chunks, and then delivers them to edge nodes. Next, edge nodes implement video processing functions on the received video chunks and upload the results to a cloud server for video analysis (such as object/event detection). A delay-sensitive video assignment is supposed to be processed within a limit and will fail if the deadline is passed. In this study, as shown in Figure \ref{fig:arch}, we consider an edge computing network, which comprises three primary components, namely, camera source node, edge node, and cloud server.
\begin{itemize}
\item Camera source node: the camera node periodically generates video tasks, divides each video task into a number of videos chunks, compresses video chunks at certain compression ratios, and then assigns compressed video chunks among all edge nodes as per scheduling policies.
\item Edge node: the edge has computational ability and storage capacity and helps preprocess video chunks. Moreover, edge nodes can form cooperative groups based on specific group formation policy and receive compressed video chunks as per the video load assignment policy.
\item Cloud Server: cloud server collects the preprocessing results from edge nodes, which has abundant computational abilities, and performs additional video analysis.
\end{itemize}
During a sparse edge node deployment, an edge node will only connect to one of the available cameras at a certain location; however, in a dense deployment, an edge node may have multiple choices on selecting multiple cameras. In this study, we focus on reducing processing load on cloud server by minimizing the video chunks that are fed from camera sources. Therefore, an edge device runs preprocessing tasks to filter uninteresting video chunks. The term “uninteresting video chunks“ is defined via scenario specification: for surveillance scenarios, it is static scenes without any objects and gradual changes.

\section{The Light-weight Runtime Moving Object Detection in Video Compressed Domain}
\begin{figure*}
\centering
 \includegraphics[width=1.0\linewidth]{Figures/arch.jpg}
 \caption{Workflow of the proposed detection-based tracking approach in compressed domain.}
 \label{fig:proposedMethod}
\end{figure*}
\begin{figure*}
\centering
\subfloat[]
{
    \includegraphics[width=0.3\linewidth]{Figures/1340.jpg}
}
\subfloat[]
{
    \includegraphics[width=0.3\linewidth]{Figures/1350.jpg}
}
\subfloat[]
{
    \includegraphics[width=0.3\linewidth]{Figures/1360.jpg}
}\\
\subfloat[]
{
    \includegraphics[width=0.3\linewidth]{Figures/3.jpg}
}
\subfloat[]
{
    \includegraphics[width=0.3\linewidth]{Figures/35.jpg}
}
\subfloat[]
{
    \includegraphics[width=0.3\linewidth]{Figures/70.jpg}
}
\caption{Video coding motion vector extraction from consecutive frames with two video test sequences.}
\label{fig:mvextract}
\end{figure*}
As mentioned in subsection \ref{subsec:0}, the aim of this method is to analyze only MVs at the edge device, recognize the real moving objects, and exclude the noised motions that are generated by environmental factors such as illumination changes and background movement, in the current video scene. For this purpose, we present a method that has a workflow shown in Figure \ref{fig:proposedMethod} to analyze the video coding MVs. In order to extract MVs from compressed bitstream, the input video stream is partially decoded and the collected MVs will include both real object MVs and noise MVs. The example of extracted MVs are represented in Figure \ref{fig:mvextract} in consecutive frames of two different video test sequences. To analyze these MVs, the following functionalities are involved.
\subsection{Median Filter and Moving Object Detection}
\label{sub:filter}
\begin{figure*}
\centering
\subfloat[]
{
    \includegraphics[scale=0.325]{Figures/1327_mv.jpg}
}
\subfloat[]
{
    \includegraphics[scale=0.3]{Figures/1350_mv.jpg}
}
\subfloat[]
{
    \includegraphics[scale=1.0]{Figures/30_mv.jpg}
}
\subfloat[]
{
    \includegraphics[scale=1.0]{Figures/10_mv.jpg}
}
\caption{Motion vector of a moving object in different video test sequences: (a, b) our record video, (c, d) Test video in VIRAT}
\label{fig:mvobject}
\end{figure*}
Compressed-domain MVs may not represent the actual of true motion due to the properties of motion estimation, biased towards efficient coding as shown in Figure \ref{fig:mvanalysis}(a,e). Therefore, it is desirable to eliminate motion vectors that can be categorised as unsuitable for moving objects detection. We assume that MVs  whose magnitudes are very high or very low, compared to the other MVs related to the moving object, has to be replaced with the median value of neighbouring MVs. Therefore, the application of vector median filter aims to reduce isolated vector noises and smoothen the difference of MV between adjacent blocks. Note that the sliding window approach is used for median filtering. Because H.264/AVC allows for variable-sized block partitioning, we construct a uniformly sample MV field by mapping all MVs to 4x4 blocks. Theoretically, the motion vector average (normalized vector) among all elements $N_{mv}$ in NxN (N=4 in this study) window function is calculated by Equation \ref{eqn:1}: 
\begin{equation}
\label{eqn:1}
N_{mv}= \frac{\sum_{i=1, j=1}^{N*N}MV_{ij}}{N*N}
\end{equation}          
Where: $MV_{ij}$ is the MV elements in the N×N window. Finally, the smoothened motion filter is presented, which is experimentally determined as shown in Figure \ref{fig:mvanalysis}(b,f) using the input MV, including isolated motion vectors noise, are smoothened in the areas that mostly correspond to the object boundaries. In order to detect the moving objects, a cluster detector is involved to determine the number of moving objects and their approximate positions in the scene. Then, a density-based cluster technique is applied to completely segment moving objects. The first step, MV whose magnitudes can be used to easily differentiate moving objects from the stationary background. However, it is difficult to directly and completely segment moving objects because not all MVs on the objects have the same motion state. While every part of a rigid body maintains nearly the same motion state, different parts of a non-rigid body can move in various ways (see Figure \ref{fig:mvobject}). In order to cluster detected MVs into dynamic clusters, a range search algorithm based on the euclidean distance of the MV point is used, under the presumption that dense points represents the same object. It requires a parameter $\varepsilon$ where $\varepsilon$ is the spatial distance threshold between density reachable MV. The goal is to partition \textit{n} MV points $\chi \subset \mathbb{R}^{d}$ into \textit{k} clusters using k-means filter. Each of the \textit{n} data points will be assigned to a cluster with the nearest mean. The mean of each cluster is called its "center". For the k-means problem, we wish to choose \textit{k} centers $\mathbb{C}$ so as to minimize the potential Equation \ref{eqn:cluster}: \\
\begin{equation}
\label{eqn:cluster}
\phi = \sum_{x \in \chi}^{}\underset{c \in \mathbb{C}}{min}\left \| x - c \right \|^{2}
\end{equation} 
From these centers, we can define a clustering by grouping data points according to which center each point is assigned to. With MV points, following steps are performed frame by frame:
\begin{itemize}
\item For each $p_{i} \in \chi$, find all the neighboring points within spatial distance $\varepsilon$.
\item Cluster all the MV points that are density reachable or density connecting \cite{ester1996density} and label them.
\item Terminate the process after all the MV points are checked. The output is a set of clusters of dynamic points.
\end{itemize}
The distance threshold $\varepsilon$ decides the range of density reachability \cite{ester1996density}.
\begin{figure*}
\centering
\subfloat[]
{
    \includegraphics[width=0.25\linewidth]{Figures/1328_mv.jpg}
}
\subfloat[]
{
    \includegraphics[width=0.25\linewidth]{Figures/1328_occ.jpg}
}
\subfloat[]
{
    \includegraphics[scale=0.3]{Figures/1328_res.jpg}
}
\subfloat[]
{
    \includegraphics[width=0.25\linewidth]{Figures/1328_obj.jpg}
}\\
\subfloat[]
{
    \includegraphics[width=0.25\linewidth]{Figures/38_mv.jpg}
}
\subfloat[]
{
    \includegraphics[width=0.25\linewidth]{Figures/38_occ.jpg}
}
\subfloat[]
{
    \includegraphics[scale=0.3]{Figures/38_res.jpg}
}
\subfloat[]
{
    \includegraphics[width=0.25\linewidth]{Figures/38_obj.jpg}
}
\caption{Moving objects detection by the proposed method with two different video test sequences:(a,e) MV extraction; (b,f) Apply median filter; (c,g) Clustering MVs; (d,f) Blob detection.}
\label{fig:mvanalysis}
\end{figure*}
After this process, the label list includes the moving objects and certain big MVs noises as shown in Figure \ref{fig:mvanalysis}(c,d). To eliminate these noises, we based the observation on the fact that noise motions because of lighting condition changes usually randomly occur without following a certain flow. Therefore, the tracking motion’s trajectories length is then used to classify the detected motion into the real motion or noise group. If the object tracking trajectory length is larger than a certain threshold, it indicates that the objects move as a flow and the time is sufficiently long to consider it as a real motion. Moreover, I-Frame does not apply motion estimation process, therefore, MV analysis process will be skipped for I-Frames. In order to overcome the discontinuously object detection process, the object tracking algotithm will also be applied to derive the moving object's bounding box in I-frame based on the last states in P-frames. As mentioned in subsection \ref{subsec:0}, the IoU-based object tracking is applied to track motion because it is light-weight and widely tracking algorithm that calculates the overlap area between two bounding objects. Note that this tracking was implemented and evaluated in some previous studies  \cite{sheu2019stam}, \cite{li2020hksiamfc}. 
\subsection{The IoU based Moving Objects Tracking}
\label{subsec:1}
To apply the IoU-based object tracking, the detected clusters are normalized with a rectangle bounding box according to the cluster's size as shown in Figure \ref{fig:mvanalysis}(d,h). The correlated regions are connected into blobs. Each blob is represented by its top-left and bottom-right corners, i.e.,  ($x_{1}, y_{1}, x_{2}, y_{2}$) may include one or many moving objects. Because the moving object's bounding box size and shape can be different comparatively frame by frame depending on MV’s intensity. Therefore, to track the moving object, an object matching algorithm based on the overlapped area of bounding boxes is applied. We assume that the existence of the real motion is continuous frame by frame. For each bounding box $B_{1}$ in the previous frame, we identify a bounding box $B_{2}$ at the current frame with the highest IoU rate. Note that IoU is attained by:\\
\begin{equation}
\label{eqn:2}
IoU(B_{1}\bigcap B_{2}) = \frac{\left \| B_{1}\bigcap B_{2} \right \|}{\left \| B_{1}\bigcup B_{2}  \right \|} =  \frac{\left \| B_{1}\bigcap B_{2} \right \|}{\left \| \left | B_{1} \right |  +  \left | B_{2} \right |  - B_{1}\bigcap B_{2} \right \|}
\end{equation} 
\begin{figure*}
\centering
 \includegraphics[width=0.4\linewidth]{Figures/iou.png}
 \caption{Object matching method}
 \label{fig:iou}
\end{figure*}
 As its definition in the Equation \ref{eqn:2}, IoU is invariant to the scale, indicating that the similarity between two arbitrary shapes A and B is independent from the scale of their space. The IoU computation’s pseudo-code is given in Algorithm \ref{alg:iou_alg}. If two bounding boxes do not overlap, the IoU value will be 0 and if the IoU score is greater than a detection score threshold ($\alpha$), two bounding boxes are considered in same account (Figure \ref{fig:iou}). The detection score threshold is determined through experiments and depend on the object velocity as well as the distance between objects and camera. Therefore, the method is run with multiple times with different detection score thresholds to tune the best value for each application scenarios. 
\begin{algorithm}
 \caption{ IoU for two bounding boxes.}
 \label{alg:iou_alg}
\SetAlgoLined
\noindent\rule{16cm}{0.4pt}\\
\KwData{ Corners of the two bounding boxes.}
- First bounding box: $A1(x_{1}, y_{1}), B1(x_{2}, y_{1}), C1(x_{2}, y_{2}), D1(x_{1}, y_{2})$\\
- Second bounding box: $A2(x'_{1}, y'_{1}), B2(x'_{2}, y'_{1}), C2(x'_{2}, y'_{2}), D2(x'_{1}, y'_{2})$\\
  where $x_{1} \leq x_{2}, y_{2} \leq y_{1} and  x'_{1} \leq x'_{2}, y'_{2} \leq y'_{1}$.\\
\textbf{Caculation}: IoU value\\
- The area of first bounding box : $Area_{1} = (x_{2} - x_{1})\times (y_{1} - x_{2})$\\
- The area of second bounding box : $Area_{2} = (x'_{2} - x'_{1})\times (y'_{1} - x'_{2})$\\
- The area of overlap: $Area_{overlap} = (max(x_{2}, x'_{2}) - min(x_{1}, x'_{1})) \times (max(y_{2}, y'_{2}) - min(y_{1}, y'_{1})) $\\
- $IoU =\frac{Area_{overlap}}{Area_{1} +  Area_{2} - Area_{overlap}}$\\
\noindent\rule{16cm}{0.4pt}
\
\end{algorithm}
\section{ Performance Evaluated Model}
Computing Resources of VA server is affected by many explicit factors. For example:  whether VA function is running, the complexity of VA function, video resolution, which kind of deep learning model used for VA function, how many cameras is serving. However, the primary factor and biggest effect is number of serving cameras and whether VA function is running. Because if  the VA server is in idle status, then other factors will be implicit. Let assume that we have a video test with N consecutive frames with K frames had the real motion (K<= N).  For each frame, VA server cost S and T unit  of average computing resource  for processing and skip frame case respectively.
For conventional method  of video analytics server, where T = S because all frames are processed , the computing resource average is:\\
\begin{equation}
\label{eqn:3}
Comp_{c}=S
\end{equation}
With our proposed method, the computing resource average during N frames  is calculated as the following equation:\\
\begin{equation}
\label{eqn:4}
Comp_{p}=\frac{(K*S+(N-K)*T)}{N}
\end{equation}
The performance ratio of two method is: \\
\begin{equation}
\label{eqn:5}
\frac{Comp_{p}}{Comp_{c}}=\frac{(K*S+(N-K)*T)}{N*S}=\frac{K}{N} + (1 - \frac{K}{N})*\frac{T}{S}
\end{equation}
if an object motion always appear in the video (K $\approx$ N), then: \\
\begin{equation}
\label{eqn:6}
 \frac{Comp_{p}}{Comp_{c}}\approx 1.
\end{equation}
In case of GPU computing resource, when VA server skip a frame then T = 0 and:\\
\begin{equation}
\label{eqn:7}
\frac{Comp_{p}}{Comp_{c}} = \frac{K}{N}
\end{equation}
 If computing resource is CPU utilization and  network throughput ,  T become very small. For example, T is used only for listening new data or connection with networking resource, then: \\
\begin{equation}
\label{eqn:8}
 \frac{Comp_{p}}{Comp_{c}} \approx \frac{K}{N}
\end{equation}